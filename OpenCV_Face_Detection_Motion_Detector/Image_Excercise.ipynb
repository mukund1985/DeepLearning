{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"rose.jpeg\",1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image is printed as 3D array as it is RBG imgage (colored)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape  # Because it is colored image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now to convert this image as gray scale i.e. 2D Array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread(\"rose.jpeg\",0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open CV converts all the images to Array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1.shape # it is gray scale image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Rose\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized = cv2.resize(img, (600,600))\n",
    "cv2.imshow(\"Rose\",resized)\n",
    "cv2.waitKey(100)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dividing Image into 2 part to decrease the size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized = cv2.resize(img, (int(img.shape[1]/2),int(img.shape[0]/2)))\n",
    "cv2.imshow(\"Rose\",resized)\n",
    "cv2.waitKey(100)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiplying Image to increase the size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized = cv2.resize(img, (int(img.shape[1]*2),int(img.shape[0]*2)))\n",
    "cv2.imshow(\"Rose\",resized)\n",
    "cv2.waitKey(10)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step 1** - Create a cascade classifier, it will contain the feature of the face. \n",
    "- **Step 2** - I will use openCV to read the image and the feature file, so it will search for the row and column values of the face numpy ndarray (The face rectangle co-ordinates).\n",
    "- **Step 3**- Display the image with the rectangular face box. \n",
    "\n",
    "\n",
    "\n",
    "- Creating a CascadeClassifier Object. Here we need to give path to the XML file which contains the face features.\n",
    "- Reading the image as it is. Here we will convert colored image to gray scale. \n",
    "- Reading the image as gray scale image.\n",
    "- Search the co-ordinates of the image. Here we will use **detectMultiScale** method to search for the face rectangle co-ordinates and with the help of **scaleFactor**, I will decrease the shape value by 5%, until the face is found. Smaller this value, the greater is the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CascadeClassifier Object\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Reading the image as it is \n",
    "img = cv2.imread('photo.jpg')\n",
    "\n",
    "# Reading the image as gray scale image\n",
    "gray_img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Search the co-ordinates of the image\n",
    "faces = face_cascade.detectMultiScale(gray_img, scaleFactor = 1.05,\n",
    "                                     mineNeighbors = 5)\n",
    "\n",
    "for x,y,w,h in faces:\n",
    "    img = cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0),3)\n",
    "    \n",
    "# reshaping the image in order to display the image properly on the screen\n",
    "\n",
    "resized = cv2.resize(img, (int(img.shape[1]/7),int(img.shape[0]/7)))\n",
    "\n",
    "cv2.imshow(\"Gray\", resized)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capturing Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will be using OpenCV for reading frames/images one-by-one**.\n",
    "\n",
    "**We will be using loops to build a window where images will appear really fast, so that you can see it as a video**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2,time\n",
    "\n",
    "# Either you can give path to the video file or use numbers. Numbers specify that you will be using the webcam \n",
    "# to capture video\n",
    "video = cv2.VideoCapture(0) # Method to create VideoCapture object, it will trigger the camera.\n",
    "                            # Here 0 is to specify that use built-in camera, 1 to specify external camera. \n",
    "\n",
    "# Here check is bool data type, returns true if Python is able to read the VideoCapture object\n",
    "# Here Frame is NumPy array, it represents the first image that video capture\n",
    "check, frame = video.read()\n",
    "\n",
    "print(check)\n",
    "print(frame)\n",
    "    \n",
    "# Lets go ahead and add time delay, using Time module amd will stop for 3 seconds.  \n",
    "time.sleep(3)\n",
    "\n",
    "# This will release the camera in some milliseconds\n",
    "video.release()\n",
    "\n",
    "# When this code will execute, you will notice that your cam light switches on for split seconds, and then it turns off. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We need to create frame object, which will read the images of the VideoCapture object.**\n",
    "\n",
    "**We will recessively show each frame of the video being captured.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2,time\n",
    "\n",
    "video = cv2.VideoCapture(0) # This will read the first frame/image of the video. \n",
    "check, frame = video.read()\n",
    "  \n",
    "# Lets go ahead and add time delay, using Time module amd will stop for 3 seconds.  \n",
    "time.sleep(3)\n",
    "\n",
    "cv2.imshow('Capturing',frame)  # imshow method is used to capture the first image/frame of the video. \n",
    "\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# This will release the camera in some milliseconds\n",
    "video.release()\n",
    "\n",
    "# When this code will execute, you will notice that your cam light switches on for split seconds, and \n",
    "# then it turns off. \n",
    "\n",
    "cv2.destroyAllWindows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to capture the Video, Instead of First Image/Frame of the Video?**\n",
    "\n",
    "Because Video is what - multiple images comes so frequently and become video. \n",
    "\n",
    "So, now In order to capture the Video, I will be using **while** loop. While Condition will be such that, untill unless `check` is True, Python will diplay the frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2,time\n",
    "\n",
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "# Number of frame that we have captured. \n",
    "a = 1\n",
    "\n",
    "#This while loop will iterate through the frames and display the window \n",
    "\n",
    "while True:\n",
    "    a = a + 1\n",
    "    check, frame = video.read()\n",
    "    print(frame)\n",
    "    \n",
    "    # Convert each frame into a gray scale image\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    cv2.imshow('Capturing',gray)\n",
    "    \n",
    "    # This will generate a new frame after every 1 miliseconds\n",
    "    key = cv2.waitKey(1)\n",
    "    \n",
    "    # Once you enter 'q' the window will be destroyed\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "print(a) # This will print the number of frames\n",
    "\n",
    "video.release()\n",
    "\n",
    "cv2.destroyAllWindows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement - \n",
    "\n",
    "You have been approached by a company that is studying human behavior. Your task is to give them a webcame, that can detect the motion or any movement in front of it. \n",
    "\n",
    "This should return a graph, this graph should contain for how long the human/object was in front of the camera. \n",
    "\n",
    "So basically we need to calculate and show that below two things in Graph - \n",
    "\n",
    "- Time, at which object appears in front of the camera. \n",
    "- Time, at which object moves away from the camera. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below are the sudo code or steps which I am going to follow to achive the above problem statement**-\n",
    "\n",
    "- Start --> Save the initial Image in a fram(**Image without object**), (as we know video is nothing but moving images) --> Convert this image to a gaussian blur image --> Calculate the difference --> Define a threshold to remove the shadows and other noises --> Define the borders of the object --> Add the rectangular box around the object --> Calculate the time, when object appears and exits the frame. \n",
    "\n",
    "There can one more situation i.e. \n",
    "\n",
    "- Start --> Take the frames **with the object** and convert it into gaussian blur image --> Calculate the difference(the difference between the first and the gaussian blur image) --> Define a threshold to remove the shadows and other noises --> Define the borders of the object --> Add the rectangular box around the object --> Calculate the time, when object appears and exits the frame. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2,time\n",
    "\n",
    "first_frame = None\n",
    "\n",
    "# Create a VideoCapture object to record video using webcam\n",
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    check, frame = video.read()\n",
    "    \n",
    "    # Convert the frame color to gray scale\n",
    "    \n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Convert the gray scale frame to GaussianBlur\n",
    "    \n",
    "    gray = cv2.GaussianBlur(gray,(21,21),0)\n",
    "    \n",
    "    # Below is used to store the first image/frame of the video \n",
    "    \n",
    "    if first_frame is None:\n",
    "        first_frame = gray\n",
    "        continue\n",
    "\n",
    "# Below piece calculates the difference between the first frame and other frames        \n",
    "delta_frame = cv2.absdiff(first_frame,gray)\n",
    "\n",
    "# Below piece will provides a threshold value, such that it will convert the difference value with \n",
    "# less than 30 to black.\n",
    "# if the difference is greater than 30 it will convert those pixels to white. \n",
    "\n",
    "thresh_data = cv2.threshold(delta_frame, 30, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "thresh_data = cv2.dilate(thresh_data, None, iterations = 0)\n",
    "\n",
    "# Below piece will Define the contour area. Basically, add the borders.\n",
    "\n",
    "(_,cnts,_) = cv2.findContours(thresh_data.copy(),cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Below loop will Remove noises and shadows. Basically, it will keep only that part white, which has \n",
    "# area greater than 1000 pixels. \n",
    "\n",
    "for contour in cnts:\n",
    "    if cv2.contourArea(contour) < 1000:\n",
    "        continue\n",
    "        \n",
    "    # Below piec will Create a rectangular box around the object in the frame. \n",
    "    \n",
    "    (x,y,w,h) = cv2.boundingRect(contour)\n",
    "    cv2.rectangle(frame, (x,y), (x+w,y+h),(0,255,0), 3)\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.imshow('Capturing', gray)\n",
    "    cv2.imshow('delta', delta_frame)\n",
    "    cv2.imshow('thresh',thresh_data)\n",
    "\n",
    "   # Frame will change in 1 millisecond\n",
    "\n",
    "   key = cv2.waitKey(1)\n",
    "\n",
    "  # This will break the loop, once the user presses 'q'\n",
    "   if key == ord('q'):\n",
    "        break\n",
    "        \n",
    "video.release()\n",
    "\n",
    "# This will close all the windows\n",
    "cv2.destroyAllWindows\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Time Values \n",
    "\n",
    "**Now we can calculate the time for which the object was in Front of the Camera**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "first_frame = None\n",
    "status_list = [None,None]\n",
    "times[]\n",
    "\n",
    "# DataFrame to store the time values during which object detection and movement appears(basically when \n",
    "# object entered into the camera and when the object went out of the camera)\n",
    "\n",
    "df = pandas.DataFrame(columns = ['Start','End'])\n",
    "\n",
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    check, frame = video.read()\n",
    "    # Status at the begining of the object recording is zero as the object is not visible. \n",
    "    status = 0\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray,(21,21),0)\n",
    "\n",
    "(_,cnts,_) = cv2.findContours(thresh_frame.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "for contour in cnts:\n",
    "    if cv2.contourArea(contour) < 10000:\n",
    "        continue\n",
    "    # Change in Status when the object is being detected\n",
    "    status = 1\n",
    "    \n",
    "    (x,y,w,h) = cv2.boundingRect(contour)\n",
    "    cv2.rectangle(frame, (x,y), (x+w, y+h),(0,255,0), 3)\n",
    "\n",
    "# List of status for every frame\n",
    "\n",
    "status_list.append(status)\n",
    "\n",
    "# Second last status value\n",
    "\n",
    "status_list = status_list[-2:]\n",
    "\n",
    "# Record datetime in a list when change occurs at last value and second last value\n",
    "\n",
    "if status_list[-1]==1 and status_list[-2]==0:\n",
    "    times.append(datetime.now())\n",
    "of status_list[-1]==0 and status_list[-2]==1:\n",
    "    times.append(datetime.now())\n",
    "    \n",
    "print(status_list)\n",
    "print(times)\n",
    "\n",
    "# Storing Time values in a DataFrame\n",
    "\n",
    "for i in range(0,len(times),2):\n",
    "    df = df.append({\"Start\":times[i],\"End\":times[i+1]},ignore_index=True)\n",
    "    \n",
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv(\"Times.csv\")\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Motion Detection Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the Data Frame from the motion_detector.py\n",
    "\n",
    "from motion_detector import df\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.models import HoverTool, ColumnarDataSource\n",
    "\n",
    "# Convert time to a string format. \n",
    "\n",
    "df[\"Start_string\"] = df[\"Start\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "df[\"End_string\"] = df[\"End\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "cds=ColumnarDataSource(df)\n",
    "\n",
    "# The DataFrame of time values is plotted on the browser using Bokes plots\n",
    "\n",
    "p = figure(x_axis_type='datetime',height=100,width=500,responsive=True,title=\"Motion Graph\")\n",
    "p.yaxis.minor_tick_line_color=None\n",
    "y.ygrid[0].ticker.desired_num_ticks=1\n",
    "\n",
    "hover=HoverTool(tooltips=[(\"Start\",\"@Start_string\"),(\"End\",\"@End_string\")])\n",
    "p.add_tools(hover)\n",
    "\n",
    "q=p.quad(left=\"Start\",right=\"End\",bottom=0,top=1,color='red',source=cds)\n",
    "\n",
    "output_file(\"Graph1.html\")\n",
    "show(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
